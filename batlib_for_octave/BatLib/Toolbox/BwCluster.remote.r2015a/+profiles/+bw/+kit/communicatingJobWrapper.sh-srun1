#!/bin/sh
# This wrapper script is intended to be submitted to SLURM to support
# communicating jobs.
#
# This script uses the following environment variables set by the submit MATLAB code:
# MDCE_CMR            - the value of ClusterMatlabRoot (may be empty)
# MDCE_MATLAB_EXE     - the MATLAB executable to use
# MDCE_MATLAB_ARGS    - the MATLAB args to use
#
# The following environment variables are forwarded through mpiexec:
# MDCE_DECODE_FUNCTION     - the decode function to use
# MDCE_STORAGE_LOCATION    - used by decode function
# MDCE_STORAGE_CONSTRUCTOR - used by decode function
# MDCE_JOB_LOCATION        - used by decode function
#
# For backward compatability, this wrapper script uses the following SLURM environment variables:
# SLURM_JOBID              - instead of SLURM_JOB_ID
# SLURM_NODELIST           - instead of SLURM_JOB_NODELIST
# SLURM_NNODES             - instead of SLURM_JOB_NUM_NODES

# Copyright 2006-2015 The MathWorks, Inc.

# Create full paths to mw_smpd/mw_mpiexec if needed
FULL_SMPD=${MDCE_CMR:+${MDCE_CMR}/bin/}mw_smpd
FULL_MPIEXEC=${MDCE_CMR:+${MDCE_CMR}/bin/}mw_mpiexec
SMPD_LAUNCHED_HOSTS=""
MPIEXEC_CODE=0
SSH_COMMAND="ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"

# Work out where we need to launch SMPDs given our hosts file - defines
# SMPD_HOSTS
chooseSmpdHosts() {
echo -e "\nchooseSmpdHosts()"

    # We need the SLURM_NODELIST value - the following line either echoes the value,
    # or aborts.
    echo Node file: ${SLURM_NODELIST:?"Node file undefined"}
    # We must launch SMPD on each unique host that this job is to run on. We need
    # this information as a single line of text, and so we pipe the output of "uniq"
    # through "tr" to convert newlines to spaces

    #
    # This will break!!!
    #
    # We're assuming the list will be of the form
    #
    #  node136,node138
    #
    # Instead, it's of the form
    #
    #  cnode[136,138],cnode[140-43],cnode157
    #
    # SMPD_HOSTS=`echo ${SLURM_NODELIST} | uniq | tr ',' ''`
    SMPD_HOSTS=`scontrol show hostname ${SLURM_NODELIST} | uniq | tr '\n', ' '`
}

# Work out which port to use for SMPD
chooseSmpdPort() {
echo -e "\nchooseSmpdPort()"

    # Choose unique port for SMPD to run on. SLURM_JOBID might be something like
    # 15.slurm-server-host.domain.com, so we extract the numeric part of that
    # using sed.
    JOB_NUM=`echo ${SLURM_JOBID:?"SLURM_JOBID undefined"} | sed 's#^\([0-9][0-9]*\).*$#\1#'`
    # Base smpd_port on the numeric part of the above
    SMPD_PORT=`expr $JOB_NUM % 10000 + 20000`
    echo "Port no: ${SMPD_PORT}"
}

# Work out how many processes to launch - set MACHINE_ARG
chooseMachineArg() {
echo -e "\nchooseMachineArg()"

# SLURM_TASKS_PER_NODE could be of the format (in the order of SLURM_NODELIST)
#
#   12                    Single node runs 12 tasks
#   12(x3)                All nodes run 12 tasks
#   7(x2),8(1)            First two nodes run 7 tasks, the third runs 8 tasks
#
#   We need to unravel this to
#
#   12
#   12 12 12
#   7 7 8
#
# respectfuly.  This is done by the stpn2tpn Python script.

    SDIR=${MDCE_STORAGE_LOCATION}/${MDCE_JOB_LOCATION}
    TPN=`${SDIR}/stpn2tpn.py "${SLURM_TASKS_PER_NODE}"`
    MACHINE_ARG="-hosts ${SLURM_NNODES}"
    idx=0
    for host in ${SMPD_HOSTS}
    do
	idx=$(expr $idx + 1)
	N=`echo $TPN | cut -f $idx -d,`
	MACHINE_ARG="${MACHINE_ARG} ${host} $N"
    done
    echo "Machine args: $MACHINE_ARG"
######################################################################

}

# Now that we have launched the SMPDs, we must install a trap to ensure that
# they are closed either in the case of normal exit, or job cancellation:
# Default value of the return code
cleanupAndExit() {
echo -e "\ncleanupAndExit()"

    echo "Stopping SMPD on ${SMPD_LAUNCHED_HOSTS} ..."

#    echo "srun --ntasks-per-node=1 --ntasks=$SLURM_JOB_NUM_NODES --cpu_bind=none ${FULL_SMPD} -shutdown -phrase MATLAB -port ${SMPD_PORT}"
#    srun --ntasks-per-node=1 --ntasks=$SLURM_JOB_NUM_NODES --cpu_bind=none ${FULL_SMPD} -shutdown -phrase MATLAB -port ${SMPD_PORT}

    echo "Exiting with code: ${MPIEXEC_CODE}"
    exit ${MPIEXEC_CODE}
}

# Use srun to launch the SMPD daemons on each processor
launchSmpds() {
echo -e "\nlaunchSmpds()"

# When calling srun, I get
#
#    srun: cluster configuration lacks support for cpu binding
#
# A Google search comes up with
#
#    "Used to resolve a problem when SLURM is configured with TaskAffinity
#     set. It's harmless, but annoying."

    # Launch the SMPD processes on all hosts using srun
    echo "Starting SMPD on ${SMPD_HOSTS} ..."

    # If we don't specify -debug, the process will come back
    # immediately, bringing the MPD down.  Set the debug level
    # to 0, because we really don't want any debugging.
    echo "srun --ntasks-per-node=1 --ntasks=$SLURM_JOB_NUM_NODES --cpu_bind=none ${FULL_SMPD} -phrase MATLAB -port ${SMPD_PORT} -debug 0&"
    srun --ntasks-per-node=1 --ntasks=$SLURM_JOB_NUM_NODES --cpu_bind=none ${FULL_SMPD} -phrase MATLAB -port ${SMPD_PORT} -debug 0&
    sleep 15

    echo "Show Ports Running Process Daemon"
    for host in ${SMPD_HOSTS}
    do
        ${SSH_COMMAND} $host netstat -ap | grep ${SMPD_PORT}
        SMPD_LAUNCHED_HOSTS="${SMPD_LAUNCHED_HOSTS} ${host}"
    done

    # Remove leading spaces
    SMPD_LAUNCHED_HOSTS=${SMPD_LAUNCHED_HOSTS## }

    # Remove trailing spaces
    SMPD_LAUNCHED_HOSTS=${SMPD_LAUNCHED_HOSTS%% }

    echo "All SMPDs launched"
}

runMpiexec() {
echo -e "\nrunMpiexec()"

    CMD="$FULL_MPIEXEC -phrase MATLAB -port $SMPD_PORT \
        -l $MACHINE_ARG -genvlist \
        MDCE_DECODE_FUNCTION,MDCE_STORAGE_LOCATION,MDCE_STORAGE_CONSTRUCTOR,MDCE_JOB_LOCATION,MDCE_DEBUG,MDCE_LICENSE_NUMBER,MLM_WEB_LICENSE,MLM_WEB_USER_CRED,MLM_WEB_ID,MDCE_MPI_EXT,TMP \
        \"${MDCE_MATLAB_EXE}\" $MDCE_MATLAB_ARGS"

    # As a debug stage: echo the command line...
    echo $CMD

    # ...and then execute it
    eval $CMD

    MPIEXEC_CODE=${?}
}

# Define the order in which we execute the stages defined above
MAIN() {
    trap "cleanupAndExit" 0 1 2 15
    chooseSmpdHosts
    chooseSmpdPort
    launchSmpds
    chooseMachineArg
    runMpiexec

    echo "Exiting with code: ${MPIEXEC_CODE}"
    exit ${MPIEXEC_CODE}
}

# Call the MAIN loop
MAIN
